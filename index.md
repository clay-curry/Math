# Preface

The construction of learning algorithms capable of modeling natural systems (e.g., protein folding, sythesis design, biomarker disovery), has motivated much work on the problem of how to constrain the space of admissible hypotheses to include only those guaranteed to respect desired symmetries. One of the most important tools for achieving this is the use of group theory, Lie groups, and Lie algebras, which allow us to fully characterize sets of transformations that leave certain systems or physical quantities invariant. While Lie theory is increasingly being used in the machine learning literature, there is often a wide gulf between the presentations of the subject: research papers that get down to business quickly but are often imprecise and inconsistent in definitions and statements of theorems, versus math books which often have an impossibly steep barrier to entry.

Using only linear algebra, Part I of the text covers the basics of linear representations and reducibility, tensor products of representations, the compact group approach to representation theory, Clebsch-Gordan Theory, and the Wigner-Eckart Theorem. Part II of this text will present the lineage of recent work in equivariant neural networks, particularly geometric graphs, and will be demonstrated using Pytorch and Pytorch Lightning. Part III is yet to be determined.

It is my hope that this book will be useful to both mathematicians, scientists, engineers, and data analysts, where the prerequisites include a basic understanding of linear algebra, multivariate calculus, and familiarity with the Python language and its machine learning libraries. .Most importantly, my hope is that someone will find these notes to be an enlightening and worthile jouney to see yet another topic in pure math diffuse into the applied.

<p>
<div style="text-align: right">Clay Curry</div>
<div style="text-align: right">Norman, OK, USA</div>
<div style="text-align: right">Feb 2023</div>
</p>

```{tableofcontents}
```
