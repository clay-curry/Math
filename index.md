# Introduction

These notes strive to provide a compact, self-contained introduction to some fascinating recent developments at the interface of group representation theory, non-commutative harmonic analysis (or Fourier Analysis), and artificial neural networks. After briefly introducing some theory, we will survey influential papers and distill core principles fueling an increasing trend to incorporte non-commutative algebraic structures in deep learning architectures, inducing crucial inductive biases for robust, data-efficient machine learning.  Importantly, these notes present a unified mathematical description of fundamental methodological developments in the construction of neural networks.

These notes are compiled for a general (mathematically-inclined) computer science audience, where the prerequisites include a basic understanding of linear algebra, multivariate calculus, and familiarity with the Python language and its machine learning libraries. My hope (not expectation) is that someone will find these notes to be an enlightening case of, yet another, topic in pure math diffuse into the applied.

<p>
<div style="text-align: right">Clay Curry</div>
<div style="text-align: right">Norman, OK, USA</div>
<div style="text-align: right">Feb 2023</div>
</p>

```{tableofcontents}
```
